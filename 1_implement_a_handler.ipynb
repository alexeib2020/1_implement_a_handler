{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-implement_a_handler.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAbY3AZ5FKZ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b8027a6d-a3d8-41b8-f374-d97e4ee82bb4"
      },
      "source": [
        "!pip install requests"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EYvKHD-CYQq"
      },
      "source": [
        "# Connectors\n",
        "\n",
        "Welcome to Capital!\n",
        "\n",
        "#### What does Capital Do?\n",
        "\n",
        "Capital uses data from third parties in order to help business owners better understand their business and finances.\n",
        "\n",
        "The more data we have, the better decisions we can make.\n",
        "\n",
        "Let's discuss how we do integrations here at Capital:\n",
        "\n",
        "### How To Implement a Connector/Integration\n",
        "\n",
        "Implementation of an integration *must* be done as follows:\n",
        "\n",
        " - data must be requested/returned one page at a time\n",
        " - handlers must respect rate limits and be runnable in a distributed environment\n",
        " - everything must be tested\n",
        "\n",
        "Let's discuss the above via examples:\n",
        "\n",
        "#### One Page At a Time\n",
        "\n",
        "Before we get any further, there are a few terms we use here that you should be familiar with:\n",
        "\n",
        " - `Task` - a task is a request for a page of data-- it is always an object\n",
        " - `Handler` - a handler is a function that executes the `Task` and returns a `TaskResult`\n",
        " - `TaskResult` - an object that returns the page of data we retrieved in the `Handler` -- it is always an object\n",
        "\n",
        "So here's the problem-- some of the data we download is huge-- many, many GBs! Too large to fit into memory.\n",
        "\n",
        "Therefore, we can't do code like the following pseudocode:\n",
        "\n",
        "```python\n",
        "\n",
        "@dataclass\n",
        "class Task:\n",
        "    name:str = \"api/data\"\n",
        "    credentials:str\n",
        "\n",
        "@dataclass\n",
        "class TaskResult:\n",
        "    task:Task\n",
        "    data:Any\n",
        "\n",
        "\n",
        "# this is the wrong way of doing things\n",
        "def handler(task:Task) -> TaskResult:\n",
        "    api_data = []\n",
        "    more_data = True\n",
        "\n",
        "    while(more_data):\n",
        "        page = 1\n",
        "        this_data = api.get_data(page = page, credentials = task.credentials)\n",
        "        api_data.append(this_data)\n",
        "        if this_data[\"has_more\"]:\n",
        "            page += 1\n",
        "        else:\n",
        "            return TaskResult(task, api_data)\n",
        "\n",
        "```\n",
        "\n",
        "The above code tries to collect _all the data at once_, which would break once the data gets sufficiently large.\n",
        "\n",
        "To get around this, we allow you to return the current page of data you just downloaded, AND a `Task` that represents that another page of data needs downloaded next:\n",
        "\n",
        "```python\n",
        "\n",
        "@dataclass\n",
        "class Task:\n",
        "    name:str = \"api/data\"\n",
        "    credentials:str\n",
        "    page:int = 1\n",
        "\n",
        "@dataclass\n",
        "class TaskResult:\n",
        "    task:Task\n",
        "    data:Any\n",
        "    next_tasks: Iterable[Task]\n",
        "\n",
        "\n",
        "# this is the correct way of doing things\n",
        "def handler(task:Task) -> TaskResult:\n",
        "    this_data = api.get_data(page = task.page, credentials = task.credentials)\n",
        "\n",
        "    # if there's more data, return a new Task to get the next page of data\n",
        "    if this_data[\"has_more\"]:\n",
        "        return TaskResult(task, this_data, [Task(dict(**task.__dict__ | {\"page\": task.page + 1}))])\n",
        "\n",
        "    # if there is no more data, return an empty list of next_tasks\n",
        "    return TaskResult(task, this_data, [])\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "#### Respect Rate Limits, Be Cool With Horizontal Scaling (Distsys)\n",
        "\n",
        "We run these `Task` requests for data, and their `Handlers` in a distributed environment. We use redis and/or pylimit to solve this problem.\n",
        "\n",
        "An example limitor that by default allows 9 requests every 1 second, and sleeps a little longer and longer until it gains access to call the api:\n",
        "\n",
        "```python\n",
        "from pylimit import PyRateLimit\n",
        "import time\n",
        "\n",
        "def limitor(namespace: str, fnn):\n",
        "    lim = PyRateLimit(period=1, limit=9)\n",
        "    sleep = [0.1, 0.4, 0.8, 1.3]\n",
        "    attempts = 0\n",
        "\n",
        "    while not lim.attempt(namespace):\n",
        "        time.sleep(sleep.pop() if len(sleep) else 2)\n",
        "        if attempts > 20:\n",
        "            raise Exception(f\"api limit exception {namespace}\")\n",
        "\n",
        "    return fnn()\n",
        "\n",
        "#\n",
        "# usage inside a handler looks like\n",
        "this_data = limitor(task.name + task.credentials, lambda: api.get_data(page = task.page, credentials = task.credentials))\n",
        "\n",
        "```\n",
        "\n",
        "With redis-centralized locking via the code above, we're totally safe to run our code in a distributed environment.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "--------\n",
        "\n",
        "\n",
        "\n",
        "# A Test -- World Trading Data\n",
        "\n",
        "Go to worldtradingdata.com and sign up for a free API key. \n",
        "\n",
        "Create a `Task` and `Handler` that download __all pages of data__ for all stocks that have \"New York\" in their name.\n",
        "\n",
        "The base for this request URL is:\n",
        "\n",
        "https://api.worldtradingdata.com/api/v1/stock_search?limit=50&page=1&api_token=[token]&search_term=new%20york"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HshstxTdBrXY"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any, Iterable\n",
        "\n",
        "\n",
        "# put your code in here\n",
        "\n",
        "@dataclass\n",
        "class Task:\n",
        "    credentials:str\n",
        "    page:int = 1\n",
        "\n",
        "@dataclass\n",
        "class TaskResult:\n",
        "    task:Task\n",
        "    data:Any\n",
        "    next_tasks: Iterable[Task]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}